{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertPreTrainedModel, AdamW, BertConfig, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torch\n",
    "import time, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "from scipy import sparse\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Train Data Set\n",
    "\n",
    "train_inputs = torch.load('./ipc_train_input256.pt')\n",
    "train_masks = torch.load('./ipc_train_mask256.pt')\n",
    "train_labels = torch.load('./train_label.pt')\n",
    "\n",
    "# load Valid Data Set\n",
    "test_inputs = torch.load('./valid_input.pt')\n",
    "test_masks = torch.load('./valid_mask.pt')\n",
    "test_labels = torch.load('./valid_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_data_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        _, pooled_output = self.bert(input_ids, attention_mask)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained('bert-base-uncased', cache_dir=None, num_labels=1383)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False, eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # AMP 적용을 위한 코드\n",
    "\n",
    "total_step = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_step/10,\n",
    "    num_training_steps=total_step\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def predict_step(model, batch, k:int, loss_fn):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, input_labels = batch\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "        scores, labels = torch.topk(loss, 5)\n",
    "        loss = loss_fn(loss, input_labels.float())\n",
    "        return torch.sigmoid(scores).cpu(), labels.cpu(), loss.cpu()\n",
    "\n",
    "def get_p_5(predict, target, top):\n",
    "    prediction = []\n",
    "    for index_list in predict:\n",
    "        predicts = [0]*target.shape[1]\n",
    "        for index in index_list[:top]:\n",
    "            predicts[index] = 1\n",
    "        prediction.append(predicts)\n",
    "    prediction = np.array(prediction)\n",
    "    target = np.array(target)\n",
    "    return np.sum(np.multiply(prediction,target))/(top*target.shape[0])\n",
    "\n",
    "def get_ndcg_5(predict, target, top):\n",
    "\n",
    "    target = sparse.csr_matrix(np.array(target))\n",
    "    log = 1.0 / np.log2(np.arange(top) + 2)\n",
    "    dcg = np.zeros((target.shape[0], 1))\n",
    "    \n",
    "    for i in range(top):\n",
    "        prediction = []\n",
    "        for index_list in predict:\n",
    "            p = index_list[i: i+1]\n",
    "            predicts = [0]*target.shape[1]\n",
    "            predicts[p] = 1\n",
    "            prediction.append(predicts)\n",
    "        prediction = sparse.csr_matrix(np.array(prediction))\n",
    "        dcg += prediction.multiply(target).sum(axis=-1) * log[i]\n",
    "        \n",
    "    return np.average(dcg / log.cumsum()[np.minimum(target.sum(axis=-1), top) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "import logzero\n",
    "logzero.setup_default_logger(logfile='./BERT_XMTC.log')\n",
    "from logzero import logger\n",
    "import os\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "model.zero_grad()\n",
    "best_p3, check_step, best_loss = 0, 0, 1.0\n",
    "        \n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "    \n",
    "    logger.info(\"\")\n",
    "    logger.info('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    logger.info('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_data_loader):    \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, input_labels = batch\n",
    "        model.train()\n",
    "        logit = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "        loss = loss_fn(logit, input_labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss: # AMP 적용을 위한 코드\n",
    "            scaled_loss.backward()\n",
    "        #loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "              \n",
    "        if step % 1000 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            check_val_loss, valid_step = 0, 0\n",
    "            p1, p3, p5 = 0.0, 0.0, 0.0\n",
    "            for batch in test_data_loader:\n",
    "                _, labels, val_loss = predict_step(model, batch, 5, loss_fn)\n",
    "                targets = batch[2].cpu()\n",
    "                check_val_loss += val_loss.item()\n",
    "                valid_step += 1\n",
    "\n",
    "                p1 += get_p_5(labels, targets, 1)\n",
    "                p3 += get_p_5(labels, targets, 3)\n",
    "                p5 += get_p_5(labels, targets, 5)\n",
    "                \n",
    "            if  best_p3 < p3:\n",
    "                path = './save_model'\n",
    "                if os.path.exists(path):\n",
    "                    model.save_pretrained(path)\n",
    "                else:\n",
    "                    os.mkdir(path)\n",
    "                    model.save_pretrained(path)\n",
    "                best_p3 = p3\n",
    "                logger.info(\"create best model\")\n",
    "                check_step = 0\n",
    "            else:\n",
    "                check_step += 1\n",
    "                if check_step >= 25:\n",
    "                    break\n",
    "            \n",
    "            print(\"{:>2} in {:>6}     train loss: {:.5f}     valid loss: {:5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, train_data_loader.batch_size*step, loss.item(), check_val_loss/valid_step, p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "            logger.info(\"{:>2} in {:>6}     train loss: {:.5f}     valid loss: {:5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, train_data_loader.batch_size*step, loss.item(), check_val_loss/valid_step, p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "\n",
    "\n",
    "        del loss, batch, input_ids, input_mask, input_labels\n",
    "\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_data_loader)\n",
    "    print(\"Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    print(\"Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "    logger.info(\"Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    check_val_loss, valid_step = 0, 0\n",
    "    p1, p3, p5 = 0.0, 0.0, 0.0\n",
    "    for batch in test_data_loader:\n",
    "        _, labels, val_loss = predict_step(model, batch, 5, loss_fn)\n",
    "        targets = batch[2].cpu()\n",
    "        check_val_loss += val_loss.item()\n",
    "        valid_step += 1\n",
    "\n",
    "        p1 += get_p_5(labels, targets, 1)\n",
    "        p3 += get_p_5(labels, targets, 3)\n",
    "        p5 += get_p_5(labels, targets, 5)\n",
    "        \n",
    "        \n",
    "    avg_valid_loss = check_val_loss/valid_step\n",
    "       \n",
    "    if  best_p3 < p3:\n",
    "        path = './save_model'\n",
    "        if os.path.exists(path):\n",
    "            model.save_pretrained(path)\n",
    "        else:\n",
    "            os.mkdir(path)\n",
    "            model.save_pretrained(path)\n",
    "        best_loss = avg_valid_loss\n",
    "        best_p3 = p3\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"create best model\")\n",
    "        check_step = 0\n",
    "    else:\n",
    "        check_step += 1\n",
    "        if check_step >= 25:\n",
    "            break\n",
    "\n",
    "    print(\"{:>2}     valid loss: {:5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, check_val_loss/valid_step, p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"{:>2}     valid loss: {:5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, check_val_loss/valid_step, p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "            \n",
    "    t0 = time.time()\n",
    "    \n",
    "    del val_loss, check_val_loss, avg_valid_loss, avg_train_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained('./save_model', cache_dir=None, num_labels=1383)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "test_inputs = torch.load('./test_input.pt')\n",
    "test_masks = torch.load('./test_mask.pt')\n",
    "test_labels = torch.load('./test_labels.pt')\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "valid_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "p1, p3, p5, valid_step = 0.0, 0.0, 0.0, 0\n",
    "for batch in tqdm(valid_data_loader):\n",
    "        labels = predict_step(model, batch, 5, loss_fn)[1]\n",
    "        targets = batch[2]\n",
    "\n",
    "        p1 += get_ndcg_5(labels, targets, 1)\n",
    "        p3 += get_ndcg_5(labels, targets, 3)\n",
    "        p5 += get_ndcg_5(labels, targets, 5)\n",
    "        valid_step += 1\n",
    "\n",
    "print(\"p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}\".format(p1/valid_step, p3/valid_step, p5/valid_step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
