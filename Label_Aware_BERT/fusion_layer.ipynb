{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertPreTrainedModel, AdamW, BertConfig, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "import time, random\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.load('../../ipc_vs_non/ipc_train_input256.pt')\n",
    "train_masks = torch.load('../../ipc_vs_non/ipc_train_mask256.pt')\n",
    "train_labels = torch.load('../../train_label.pt')\n",
    "test_inputs = torch.load('../../ipc_vs_non/valid_input.pt')\n",
    "test_masks = torch.load('../../ipc_vs_non/valid_mask.pt')\n",
    "test_labels = torch.load('../../ipc_vs_non/valid_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.load('../../ipc_vs_non/ipc_train_inputs512.pt')\n",
    "train_masks = torch.load('../../ipc_vs_non/ipc_train_masks512.pt')\n",
    "train_labels = torch.load('../../train_label.pt')\n",
    "test_inputs = torch.load('../../ipc_vs_non/valid_input.pt')\n",
    "test_masks = torch.load('../../ipc_vs_non/valid_mask.pt')\n",
    "test_labels = torch.load('../../ipc_vs_non/valid_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_data_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 1\n",
    "device = torch.device(f'cuda:{GPU_NUM}')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_emb=np.zeros((1383,768))\n",
    "\n",
    "with open('../../label768_wv', 'r') as f:\n",
    "    for index, i in enumerate(f.readlines()):\n",
    "        if index==0:\n",
    "            continue\n",
    "        i = i.rstrip('\\n')\n",
    "        n = i.split(' ')[0]\n",
    "        content = i.split(' ')[1:]\n",
    "        label_emb[int(n)] = [float(value) for value in content]\n",
    "\n",
    "label_emb = torch.from_numpy(label_emb).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, labels_num, label_emb):\n",
    "        super(LabelAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = labels_num\n",
    "        self.label_emb = label_emb\n",
    "\n",
    "        label_embedding = torch.FloatTensor(self.num_labels,self.hidden_size)\n",
    "\n",
    "        if self.label_emb is None:\n",
    "            nn.init.xavier_normal_(label_embedding)\n",
    "        else:\n",
    "            label_embedding.copy_(self.label_emb)\n",
    "        \n",
    "        self.label_embedding = nn.Parameter(label_embedding,requires_grad=False)\n",
    "        \n",
    "        self.key_layer = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        nn.init.xavier_uniform_(self.key_layer.weight)\n",
    "        \n",
    "        self.query_layer = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        nn.init.xavier_uniform_(self.query_layer.weight)\n",
    "        \n",
    "    def forward(self, inputs, masks):\n",
    "        \n",
    "        attn_key = self.key_layer(inputs).transpose(1,2)\n",
    "        \n",
    "        label_emb = self.label_embedding.expand((attn_key.size(0),self.label_embedding.size(0),self.label_embedding.size(1)))\n",
    "        attn_query = self.query_layer(label_emb)\n",
    "        \n",
    "        attention = torch.bmm(label_emb, attn_key).masked_fill(~masks, -np.inf)\n",
    "        attention = F.softmax(attention, -1)\n",
    "        \n",
    "        return torch.bmm(attention, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, labels_num):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.labels_num = labels_num\n",
    "        \n",
    "        self.attention = nn.Linear(self.hidden_size, self.labels_num, bias=False)\n",
    "        nn.init.xavier_uniform_(self.attention.weight)\n",
    "        \n",
    "    def forward(self, inputs, masks):\n",
    "        attention = self.attention(inputs).transpose(1,2).masked_fill(~masks, -np.inf)\n",
    "        attention = F.softmax(attention, -1)\n",
    "        \n",
    "        return torch.bmm(attention, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLinear(nn.Module):\n",
    "    def __init__(self, hidden_size, label_num):\n",
    "        super(MLinear, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.label_num = label_num\n",
    "        \n",
    "        self.linear_weight1 = nn.Linear(self.hidden_size,1)\n",
    "        nn.init.xavier_uniform_(self.linear_weight1.weight)\n",
    "        \n",
    "        self.linear_weight2 = nn.Linear(self.hidden_size,1)\n",
    "        nn.init.xavier_uniform_(self.linear_weight2.weight)\n",
    "        \n",
    "        self.fusion_linear = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        nn.init.xavier_uniform_(self.fusion_linear.weight)\n",
    "        \n",
    "        #self.relu = nn.LeakyReLU(0.1)\n",
    "        self.ln = nn.LayerNorm([self.label_num, self.hidden_size])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.out_linear = nn.Linear(self.hidden_size, 1)\n",
    "        nn.init.xavier_uniform_(self.out_linear.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, self_attn, label_attn):\n",
    "        factor1 = torch.sigmoid(self.linear_weight1(self_attn))\n",
    "        factor2 = torch.sigmoid(self.linear_weight2(label_attn))\n",
    "        factor1 = factor1 / (factor1+factor2)\n",
    "        factor2 = 1 - factor1\n",
    "        \n",
    "        out1 = factor1 * self_attn #[batch, label, hidden]\n",
    "        out2 = factor2 * label_attn #[batch, label, hidden]\n",
    "        \n",
    "        out = torch.cat((out1, out2), dim=-1)\n",
    "        \n",
    "        out = self.fusion_linear(out)\n",
    "        out = self.ln(out)\n",
    "        out = F.gelu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out_linear(out)\n",
    "        \n",
    "        return torch.squeeze(out, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, label_emb):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config, label_emb=None)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.label_emb = label_emb\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.self_attn = SelfAttention(self.hidden_size, self.num_labels)\n",
    "        self.label_attn = LabelAttention(self.hidden_size, self.num_labels, self.label_emb)\n",
    "        self.linear = MLinear(self.hidden_size, self.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        sequence, _ = self.bert(input_ids, attention_mask)\n",
    "        sequence = self.dropout(sequence) # [batch, sequence, hidden_size]\n",
    "        \n",
    "        masks = attention_mask  != 0 # [batch, sequence]\n",
    "        masks = torch.unsqueeze(masks, 1) # [batch, 1, sequence]\n",
    "        \n",
    "        self_attn = self.self_attn(sequence, masks)\n",
    "        label_attn = self.label_attn(sequence, masks)\n",
    "        \n",
    "        return self.linear(self_attn, label_attn)\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained('bert-base-uncased', cache_dir=None, num_labels=1383, label_emb=label_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True, smooth=0):\n",
    "        super(NormalizedFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        if self.smooth != 0:\n",
    "            targets = (1-self.smooth) * targets + self.smooth / inputs.size(1)\n",
    "            \n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_term = (1-pt).pow(self.gamma)\n",
    "        normalize = 1/focal_term.mean()\n",
    "        F_loss = normalize * self.alpha * focal_term * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=7e-5, correct_bias=False, eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # AMP 적용을 위한 코드\n",
    "\n",
    "total_step = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_step/10,\n",
    "    num_training_steps=total_step\n",
    ")\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "loss_fn = NormalizedFocalLoss(alpha=0.25).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def predict_step(model, batch, k:int, loss_fn):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, input_labels = batch\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "        scores, labels = torch.topk(loss, 5)\n",
    "        loss = loss_fn(loss, input_labels.float())\n",
    "        return torch.sigmoid(scores).cpu(), labels.cpu(), loss.cpu()\n",
    "\n",
    "def get_p_5(predict, target, top):\n",
    "    prediction = []\n",
    "    for index_list in predict:\n",
    "        predicts = [0]*target.shape[1]\n",
    "        for index in index_list[:top]:\n",
    "            predicts[index] = 1\n",
    "        prediction.append(predicts)\n",
    "    prediction = np.array(prediction)\n",
    "    target = np.array(target)\n",
    "    return np.sum(np.multiply(prediction,target))/(top*target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "import logzero\n",
    "logzero.setup_default_logger(logfile='./fusion.log')\n",
    "from logzero import logger\n",
    "import os\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "model.zero_grad()\n",
    "best_p5, check_step, best_loss = 0, 0, 1.0\n",
    "        \n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "    \n",
    "    logger.info(\"\")\n",
    "    logger.info('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    logger.info('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "        \n",
    "    if epoch_i > 40:\n",
    "        for step, batch in enumerate(train_data_loader):\n",
    "            scheduler.step()\n",
    "    else:\n",
    "        for step, batch in enumerate(train_data_loader):    \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, input_labels = batch\n",
    "            model.train()\n",
    "            logit = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "            loss = loss_fn(logit, input_labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss: # AMP 적용을 위한 코드\n",
    "                scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            if step % 1000 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                check_val_loss, valid_step = 0, 0\n",
    "                p1, p3, p5 = 0.0, 0.0, 0.0\n",
    "                for batch in test_data_loader:\n",
    "                    _, labels, val_loss = predict_step(model, batch, 5, loss_fn)\n",
    "                    targets = batch[2].cpu()\n",
    "                    check_val_loss += val_loss.item()\n",
    "                    valid_step += 1\n",
    "\n",
    "                    p1 += get_p_5(labels, targets, 1)\n",
    "                    p3 += get_p_5(labels, targets, 3)\n",
    "                    p5 += get_p_5(labels, targets, 5)\n",
    "\n",
    "\n",
    "                print(\"{:>2} in {:>6}     train loss: {:.5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, train_data_loader.batch_size*step, loss.item(), p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "                logger.info(\"{:>2} in {:>6}     train loss: {:.5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, train_data_loader.batch_size*step, loss.item(), p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "\n",
    "\n",
    "            del loss, batch, input_ids, input_mask, input_labels\n",
    "\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_data_loader)\n",
    "        print(\"Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "        print(\"Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "        logger.info(\"Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        check_val_loss, valid_step = 0, 0\n",
    "        p1, p3, p5 = 0.0, 0.0, 0.0\n",
    "        for batch in test_data_loader:\n",
    "            _, labels, val_loss = predict_step(model, batch, 5, loss_fn)\n",
    "            targets = batch[2].cpu()\n",
    "            check_val_loss += val_loss.item()\n",
    "            valid_step += 1\n",
    "\n",
    "            p1 += get_p_5(labels, targets, 1)\n",
    "            p3 += get_p_5(labels, targets, 3)\n",
    "            p5 += get_p_5(labels, targets, 5)\n",
    "\n",
    "\n",
    "        avg_valid_loss = check_val_loss/valid_step\n",
    "\n",
    "        if  best_p5 < p5:\n",
    "            path = './fusion'\n",
    "            if os.path.exists(path):\n",
    "                model.save_pretrained(path)\n",
    "            else:\n",
    "                os.mkdir(path)\n",
    "                model.save_pretrained(path)\n",
    "            best_loss = avg_valid_loss\n",
    "            best_p5 = p5\n",
    "            check_step = 0\n",
    "        else:\n",
    "            check_step += 1\n",
    "            if check_step >= 25:\n",
    "                break\n",
    "\n",
    "        print(\"{:>2}    valid loss: {:.5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, avg_valid_loss, p1/valid_step, p3/valid_step, p5/valid_step, check_step))\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"{:>2}    valid loss: {:.5f}     p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}     check_step : {:>2}\".format(epoch_i, avg_valid_loss, p1/valid_step, p3/valid_step, p5/valid_step, check_step))            \n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        del val_loss, check_val_loss, avg_valid_loss, avg_train_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained('./fusion', cache_dir=None, num_labels=1383, label_emb=label_emb)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "test_inputs = torch.load('./test_input.pt')\n",
    "test_masks = torch.load('./test_mask.pt')\n",
    "test_labels = torch.load('./test_labels.pt')\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "valid_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "p1, p3, p5, valid_step = 0.0, 0.0, 0.0, 0\n",
    "for batch in tqdm(valid_data_loader):\n",
    "        labels = predict_step(model, batch, 5, loss_fn)[1]\n",
    "        targets = batch[2]\n",
    "\n",
    "        p1 += get_p_5(labels, targets, 1)\n",
    "        p3 += get_p_5(labels, targets, 3)\n",
    "        p5 += get_p_5(labels, targets, 5)\n",
    "        valid_step += 1\n",
    "\n",
    "print(\"p1 : {:.5f}     p3 : {:.5f}     p5 : {:.5f}\".format(p1/valid_step, p3/valid_step, p5/valid_step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
